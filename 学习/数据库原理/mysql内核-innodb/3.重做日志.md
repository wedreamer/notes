# 第四章 重做日志

## 相关概念

重做日志用来实现事务的持久性, 即事务 ACID 中的 D. 重做日志由两部分组成:
- 内存中的日志缓冲(redo log buffer), 易丢失
- 重做日志文件(redo log file), 持久

innodb 是事务的存储引擎, 通过 force log at commit 机制实现事务的持久性. 当事务 commit 时, 必须先将事务的所有日志写入到重做日志文件进行持久化, 待完成后事务 commit 操作才算完成, 而这里的日志是指重做日志. 

innodb 中的日志
- redo log -> 保证事务的持久性
- undo log -> 事务回滚以及 MVCC 功能

redo log 基本上都是顺序写的, 在数据库运行时不需要堆 redo log 的文件进行读取操作
undo log 是需要进行随即读写的

为了确保每次日志都写入到重做日志文件, 每次将重做日志缓冲写入到重做日志文件后, innodb 都需要调用一次 fsync 操作. 因为重做日志文件打开并没有使用 O_DIRECT 选项, 所以重做日志先写入到文件系统缓存. 为了确保重做日志写入到磁盘, 必须进行一次 fsync 操作. 由于 fsync 的效率取决于磁盘的性能, 因此磁盘的性能决定了事务提交的性能, 也就是数据库的性能.

参数 innodb_flush_log_at_trx_commit 用来控制重做日志刷新到磁盘的策略. 0 或者 1.
- 0 表示事务提交时并不强制一定要写入到重做日志, 这个操作仅在 master thread 中完成. 而 master thread 中每 1 秒会进行依次重做日志文件的 fsync 操作. 因此当发生 mysql 数据库宕机时, 可能会发生最后一秒内事务丢失的情况. 将该参数设置为 0 时, innodb 就不再符合事务持久性的要求.
- 1 默认, 每次事务都同步到重做日志文件.
- 2 事务提交时将重做日志写入到重做日志文件, 但仅写入到文件系统的缓存中, 不进行 fsync 操作. 此时缓存丢失取决于操作系统宕机.

二进制日志 bin log, 用来进行 point-in-time(PIT) 的恢复以及主从复制(replication) 环境的建立. 

redo log 是 innodb 产生的, bin log 是 mysql 上层产生的. bin log 适用于所有的存储引擎. 

bin log 是逻辑日志, 记录对应的 sql 语句. redo log 是物理逻辑个是日志, 记录的是对于每个页的更改.
bin log 只在事务提交完成后进行一次写入, 而 redo log 是在事务进行中不断被写入. 表现为日志并不是随事务提交顺序而进行写入的.

![二进制日志与重做日志写入的时间点不同](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657655874463-11.png)  

redo log 记录的是页物理逻辑操作日志, 因此每个事务对应多个日志条目, 并且事务的重做日志写入是并发的, 并发在事务提交时写入. 所以文件中记录的树勋并非是事务开始的顺序. 

### 物理逻辑日志

redo log 类型
- 物理日志
- 逻辑日志
- 物理逻辑日志

物理日志保存一个页中发生改变的字节, old value-new value logging, 具有幂等性, 问题是产生的量比较大. 比如一个页的重新整理, 日志大小可能就是页的大小. 如果一个操作涉及多个页的修改, 就需要分别对不同页进行记录.

逻辑日志记录的是对表的操作, 类似于 bin log. 大小比较小.

undo 操作仅需对记录的日志操作进行逆操作. insert -> delete, delete -> insert. 缺点就是无法保证数据的一致性. 比如对表进行插入操作时, 表上还有其他辅助索引. 当操作未完成时系统发生了宕机, 要回滚可能很困难.

物理逻辑日志结合了物理日志与逻辑日志各自优点. 设计思想: physical-to-a-page, logical-within-a-page (对页时物理的, 页内部的操作时逻辑的).

### LSN

log sequence number. 代表的时日志序列号. lsn 占用 8 个字节, 使用 dulint_struct 结构来存储, 单调递增, 代表每个重做日志的编号. 

lsn 存在多个对象中, 表示的含义各不相同
- redo log
- check point
- page

lsn 表示事务写入 redo log 的字节总量. 已经写入到 redo log cache 和 redo log 两部分 lsn 信息.

lsn 还记录在每个页中, 在每个页的头部, 有一个值 FIL_PAGE_LSN, 表示该页最后刷新时 lsn 的大小. 因为 redo log 记录的时每个页的日志, 因此页中的 lsn 用来判断页是否需要进行恢复操作.
redo log 中 lsn 大于 page 的 lsn 表示需要重做.

检查点也通过 lsn 的形式来保存, 其表示页已经刷新到磁盘的 lsn 位置, 带数据库重启时, 仅需从检查点开始进行恢复操作. 

### 检查点

innodb 为了实现事务的持久性, 采用 write ahead log 策略, 即当事物提交时, 首先将 redo log 写入到文件, 实际数据页刷新到磁盘的操作由 checkpoint 负责. 当宕机或者其他意外导致数据丢失时, 通过 redo log 来完成呢个数据的恢复.

虽然事务的日志已经在提交时确定写入到磁盘, 但是 buffer poll 中页并没有刷新到磁盘. 这是因为事务提交仅仅是吧事务操作所涉及页的重做日志都写入磁盘. 页的刷新是异步的, 当前数据库通常使用检查点技术. 

检查点所作的操作就是将 buffer poll 中的 page 刷新到磁盘, 最后总达到外存和内存中的 page 数据一致. 检查点的作用是为了缩短数据库宕机恢复所需要的时间.

![checkpoint 的一个例子](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657658314492-15.png)  

innodb 中的两种检查点
- sharp checkpoint
- fuzzy checkpoint

sharp checkpoint 是指将 buffer pool 中修改的 page, 也就是赃页全部刷新到磁盘. 缺点就是在进行 sharp checkpoint 时不能进行其他的 DML 操作. 对生产环境数据库, 是难以接受的方式.

fuzzy checkpoint 是将赃页慢慢刷新回磁盘, 大大提高数据库的可用性. 然而这样做的前提是赃页需要根据第一次被修改时的 lsn 进行排序, 然后将最老的页由县刷新到磁盘. 保证 page 的刷新和日志的 lsn 顺序是一样的, 从而保证恢复操作的正确性.

### 归档日志

重做日志文件的大小是固定的, 如果 redo log file 的个数为 3, 每个 redo log file 大小为 1 GB, 那么整个重做日志组大小为 3 GB. 如果写入的日志大于 3 GB, 就需要循环使用重做日志, 这种方式为 round robin.

归档日志中的 redo log 与 redo log file 的内容完全相同.

### 恢复

不管赏赐数据库运行是否正常关闭, 都会尝试进行恢复. 因为 redo log 是物理日志, 所以恢复速度比逻辑日志, 如 bin log 要快的多. 恢复优化使用顺序读取以及并行应用 redo log, 加快恢复速度.

恢复会根据 checkpoint 来确定范围.

## 物理存储结构

### 重做日志物理结构

redo log 
- redo log buffer
- redo log group
- redo log file
- 归档重做日志文件

每个 redo log group 存储的内容都是完全相同的, 是镜像关系, 目的是提高数据库的可用性. 

![redo log 存储结构](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657659104399-18.png)  

redo log buffer 存放内存, 容易丢失. 大小由参数 innodb_log_buffer_size 控制, 默认大小 1MB.

当一组中的重做日志发生介质损坏时, innodb 依旧可以提供服务.

用户可以通过建立荣誉磁盘阵列(RAID) 来保证数据库服务的搞可用性. 避免在 innodb 增加额外的磁盘开销.

redo log file 默认文件名前缀 ib_logfile. redo log file 总大小必须小于 4 GB.

redo log file 是循环使用的. 归档可以对重做日志进行备份. 默认前缀名为 ib_arch_log_.

### 重做日志块

redo log 都是以 512 byte 进行存储的. 这一位这 redo log buffer, redo log file 以及归档重做日志都是以  block 的方式进行保存的. 称为 redo log block.

如果一个 page 产生的 redo log 数量大于 512 byte, 需要分割为多个 redo log block 进行存储. 由于 redo log block 大小和磁盘扇区大小一样, 都是 512 byte, 因此 redo log 的写入可以保证原子性, 不需要 doublewrite.

redo log block
- log clock header
- log body
- log block tailer

![redo log block 结构](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657659786650-3.png)  

### 重做日志组与文件

redo log file 存储 redo log block. log buffer 根据一定规则将内存中的 log block 刷新到磁盘.

- 事务提交时
- 写入检查点值
- 当 log buffer 中有一使用空间超过某个阈值时

虽然 log block 总是在 redo log file 最后部分写入, 但是并不是顺序写的. 因为 redo log file 除了保存从 log buffer 刷新到磁盘的 log block, 还保存了一些其他信息, 该信息一共占用 2 KB 大小, 每个重做日志文件的前 2 KB 部门不保存 log block 信息. 保存了 4 个 512 byte 大小的块.

- log file header
- checkpoint1
- null
- checkpoint2

每个 redo log group 的第一个 redo log file 文件上. 仅保留这些空间, 不实际保存上述信息. 写入的时候不仅需要 log block 写入还需要更新前 2 KB 部分的信息. 这些信息对 InnoDB 的恢复十分重要.

![log group 与 redo log file 之间的关系](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657660381593-36.png)  

log file header

log file header 保存了对于该 log group 的一些基本信息.

- LOG_GROUP_ID
> redo log group 的 ID 号
- LOG_FILE_START_LSN
> 每个 redo log file 第一个日志的 lsn

checkpoint

设计为交替写入, 避免介质失败, 导致无法找到可用的 checkpoint. 

## 相关数据结构

- log_group_struct
- log_struct
- 

![各种 lsn 距离](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657660751116-7.png)  

## 组提交

> 一个事务提交时都需要进行一次  fsync 操作, 以确保事务所需的重做日志都已经持久化保存到磁盘上. 然而磁盘 fsync 的性能有限, 为了提高数据库提交时的性能, 数据库允许将一组事务进行提交, 称之为组提交(group commit)

当 redo log buffer 刷新到 redo log file 时, innodb 会对最后一个 redo log block 进行复制.
如此处理, 当重做日志在写入磁盘时, 允许之后的事务的重做日志继续写入到赏赐的重做日志缓冲处, 一次实现 group commit. (进行 redo log file 的 fsync 操作时, 会释放 log_sys->mutex)
对于 redo log file 的写入, 都是缓存写的, 没有使用 O_DIRECT 选项. 时为了进一步提高 group commit 的效率.

1. lock log->mutex
2. copy last redo log block to a new block
3. write redo log buffer to disk
4. unlock log->mutex
5. fsync redo log files

group commit 在并发下效果会非常显著, 对于线程数低于 3 个的, 组提交不会带来任何的性能提升. 

## 恢复

### 数据结构

recv_sys_struct 用来管理 redo log 的恢复操作. 哈希表由多个 bucket 组成, 每个 bucket 中存放的时 recv_add_t 数据结构, 根据 (space, page_no) 进行哈希, 哈希值相同的 (space, page_no) 放入到同一个 bucket 中, 并用链表进行串联. recv_add_t 存放对应 (space, page_no) page 中的 redo log  recv_t. recv_t 中记录了 redo log 的类型, 长度, 开始 lsn, 结束 lsn, 以及 redo log 的 body. body 由 recv_data_t 数据结构定义. 由于每个 redo log body 的最大长度为 16 KB, 对于大于该长度的 redo log, 同样需要进行链接.

![恢复时哈希表中存放的数据结构](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657662167552-46.png)  

### 重做日志恢复

调用函数 recovery_from_checkpoint_start() 尝试恢复操作.

区分数据库是否需要恢复操作
表空间第一个 page (0, 0) 是一个比较特殊的 page, 仅在该 page 的 FIL_HEADER 中定义了 FIL_PAGE_FILE_FLUSH_LSN. 该值记录了书距库关闭时最后刷新页的 lsn. 如果正常关闭, 该值应该和 redo log 中保存的检查点值相等. (fil_write_flushed_lsn_to_data_files 函数完成)

![innodb 存储引擎恢复过程中函数的调用关系](https://shubuzuo.coding.net/p/image-host/d/image-host/git/raw/master/images/2022-07/2022-07-13/pic_1657662513396-48.png)  

recv_recovery_from_checkpoint_start 
> 查找检查点值
-> recv_find_max_checkpoint 
> 得到最大值
-> log_group_read_checkpoint_info 
-> recv_group_scan_log_recs 
-> log_group_read_log_seg 
-> recv_scan_log_recs 
-> recv_sys_add_to_parsing_buf 
-> recv_parse_log_recs 
-> recv_parse_org_apply_log_rec_body 
-> recv_add_to_hash_table 
-> recv_apply_hashed_log_recs 
-> recv_sys_justify_left_parsing_buf

恢复区间应为 (checkpoint, last_redo_lsn). 在恢复上述区间日志的数据时, 通过小批量的方式读取重做日志文件中的 log block 到 log_sys->buf 中, 每次读取的量为 64 KB， 就是 64K/512 = 128 个 redo log block. 在读到 redo log 的 redo log block 后, innodb 会对其进行分析, 判断 redo log block 中的日志是否包含上一个日志的内容. 因为一个日志可能存放于多个 redo log block 中. 再将日志复制到 recv_sys->buf 对象中后, 就可以对日志进行分析, 之后根据日志对应的 (space, offset) 的哈希值插入到 recv_sys 的哈希表中, recv_sys->addr_hash, 根据哈希表中的日志进行 page 恢复.

如果 recv_sys 保存恢复的日志占用过多的内存空间时, 会强制进行 page 的恢复操作.

1. recv_parse_or_apply_log_rec_body

根据 redo log 的类型, 调用对应的函数进行恢复操作.

2. recv_add_to_hash_table

3. recv_recover_page

4. recv_read_in_area
